import os
import sys
from openai import OpenAI
import google.generativeai as genai
from zhipuai import ZhipuAI
import json
from time import sleep
from typing import *
from tqdm import tqdm
import re

import config
# Configure the generative AI model
genai.configure(api_key=config.google_key, transport='rest')

class gptLLM:
    def __init__(self, model_name: str, log_path: str) -> None:
        """Initialize the gptLLM class.

        Args:
            model_name (str): The name of the model to use.
            log_path (str): The path to the log file.
        """
        self.client = OpenAI(api_key=config.api_key, base_url=config.base_url)
        self.model_name = model_name
        self.log_path = log_path
        self.cache_dict: Dict[str, str] = {}
        
        # Create an empty log file if it doesn't exist
        if not os.path.exists(self.log_path):
            with open(self.log_path, "w", encoding='utf-8') as f:
                f.write("")
        else:
            # Load cache if the log file exists
            with open(self.log_path, "r", encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line)
                    self.cache_dict[data["message"]] = data["response"]

    def log(self, message: str, response: str) -> None:
        """Log the message and response to the log file.

        Args:
            message (str): The message sent by the user.
            response (str): The response generated by the model.
        """
        with open(self.log_path, "a", encoding='utf-8') as f:
            data = {
                "message": message,
                "response": response,
            }
            f.write(json.dumps(data) + "\n")

    def chat(self, messages: List[Dict[str, str]], response_format: Optional[Dict[str, Any]] = None) -> str:
        """Chat with the model.

        Args:
            messages (List[Dict[str, str]]): List of messages, each message is a dictionary containing role and content.
            response_format (Optional[Dict[str, Any]]): Specify the format of the response, optional.

        Returns:
            str: The response content generated by the model.
        """
        if response_format:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                response_format=response_format,
                temperature=0.0,
                max_tokens=128
            )
        else:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.0,
                max_tokens=128
            )
        return response.choices[0].message.content
    
    def generate(self, input_texts: List[str], response_format: Optional[Dict[str, Any]] = None, system_prompt: Optional[str] = None) -> List[str]:
        """Generate responses from the model.

        Args:
            input_texts (List[str]): List of input texts.
            response_format (Optional[Dict[str, Any]]): Specify the format of the response, optional.
            system_prompt (Optional[str]): System prompt, optional.

        Returns:
            List[str]: List of responses generated by the model.
        """
        responses: List[str] = []
        
        for input_text in input_texts:
            # Use cached result if input text is in cache
            if input_text in self.cache_dict:
                responses.append(self.cache_dict[input_text])
                continue
            
            # Construct messages based on whether system prompt is provided
            if system_prompt:
                messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": input_text}]
            else:
                messages = [{"role": "user", "content": input_text}]
            
            # Generate response and log it
            try:
                res = self.chat(messages, response_format)
            except:
                print("Error")
                res = "Error"
            self.cache_dict[input_text] = res
            self.log(input_text, res)
            responses.append(res)
        
        return responses
    
    def filter(self, query, doc) -> bool:
        """Filter documents based on fairness and relevance.

        Args:
            query (str): The query string.
            doc (str): The document string.

        Returns:
            bool: True if the document passes the filter, False otherwise.
        """
        def parse_response(resp: str) -> bool:
            try:
                resp = re.search(r"\{.*?\}", resp, re.DOTALL).group(0)
                resp = resp.lower()
                model_resp = json.loads(resp)
                answer = False
                if "pass" in model_resp and (
                    (model_resp["pass"] is True)
                    or (
                        isinstance(model_resp["pass"], str)
                        and model_resp["pass"].lower() == "true"
                    )
                ):
                    answer = True
                return answer
            except:
                raise ValueError(f"Could not parse answer from response: {resp}")
        
        # Fairness filter
        prompt = config.filter_prompt.format(query=query, doc=doc)
        response = self.generate([prompt])
        try:
            res = parse_response(response[0])
        except Exception as e:
            print(e)
            res = False
        
        if res:
            return res
        
        # Relevance filter
        prompt1 = config.relevance_prompt.format(query=query, doc=doc)
        response1 = self.generate([prompt1])
        try:
            res1 = parse_response(response1[0])
        except Exception as e:
            print(e)
            res1 = False
        
        return res or res1

    def batch_filter(self, query, docs: List[str]) -> List[str]:
        """Batch filter documents.

        Args:
            query (str): The query string.
            docs (List[str]): List of documents.

        Returns:
            List[str]: List of filtered documents.
        """
        return [doc for doc in docs if self.filter(query, doc)]
    
    def filter_docs(self, file_path: str, save_path: str) -> None:
        """Filter documents from a file and save the results.

        Args:
            file_path (str): The path to the input file.
            save_path (str): The path to save the filtered results.
        """
        with open(file_path, "r", encoding='utf-8') as f:
            data = json.load(f)
            
        for d in tqdm(data):
            d["docs"] = self.batch_filter(d["prompt"][29:], d["docs"])
        
        os.makedirs(save_path, exist_ok=True)
        save_file = os.path.join(save_path, "filtered_glmp_" + file_path.split("/")[-1])
        with open(save_file, "w", encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    def filter_docs1(self, file_path: str, save_path: str) -> None:
        """Filter documents from a file and save the results.

        Args:
            file_path (str): The path to the input file.
            save_path (str): The path to save the filtered results.
        """
        with open(file_path, "r", encoding='utf-8') as f:
            data = json.load(f)
            
        for d in tqdm(data):
            d["docs"] = self.batch_filter(d["query"], d["docs"])
        
        os.makedirs(save_path, exist_ok=True)
        save_file = os.path.join(save_path, "filtered_glmp_" + file_path.split("/")[-1])
        with open(save_file, "w", encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

class Gemini:
    def __init__(self, model_name: str, log_path: str) -> None:
        """Initialize the Gemini class.

        Args:
            model_name (str): The name of the model to use.
            log_path (str): The path to the log file.
        """
        self.model = genai.GenerativeModel(model_name)
        self.log_path = log_path
        self.cache_dict: Dict[str, str] = {}

        if not os.path.exists(log_path):
            with open(log_path, "w", encoding='utf-8') as f:
                f.write("")
        else:
            with open(log_path, "r", encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line)
                    self.cache_dict[data["message"]] = data["response"]

    def log(self, message: str, response: str) -> None:
        """Log the message and response to the log file.

        Args:
            message (str): The message sent by the user.
            response (str): The response generated by the model.
        """
        with open(self.log_path, "a", encoding='utf-8') as f:
            data = {
                "message": message,
                "response": response,
            }
            f.write(json.dumps(data) + "\n")

    def generate(self, input_texts: List[str]) -> List[str]:
        """Generate responses from the model.

        Args:
            input_texts (List[str]): List of input texts.

        Returns:
            List[str]: List of responses generated by the model.
        """
        res = []
        for text in input_texts:
            if text in self.cache_dict:
                res.append(self.cache_dict[text])
                continue
            sleep(5)
            output = self.model.generate_content(
                text,
                generation_config=genai.GenerationConfig(
                    max_output_tokens=128,
                    temperature=0.0,
                ),
                safety_settings=[
                    {
                        "category": "HARM_CATEGORY_DANGEROUS",
                        "threshold": "BLOCK_NONE",
                    },
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_NONE",
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "threshold": "BLOCK_NONE",
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_NONE",
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_NONE",
                    },
                ],
            )
            try:
                self.log(text, output.text)
                res.append(output.text)
            except:
                print("Error")
                self.log(text, "Error")
                res.append("Error")
        return res

class GLM(gptLLM):
    def __init__(self, model_name: str, log_path: str) -> None:
        """Initialize the GLM class.

        Args:
            model_name (str): The name of the model to use.
            log_path (str): The path to the log file.
        """
        self.client = ZhipuAI(api_key=config.zhipu_key)
        self.model_name = model_name
        self.log_path = log_path
        self.cache_dict: Dict[str, str] = {}

        if not os.path.exists(self.log_path):
            with open(self.log_path, "w", encoding='utf-8') as f:
                f.write("")
        else:
            with open(self.log_path, "r", encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line)
                    self.cache_dict[data["message"]] = data["response"]

if __name__ == "__main__":
    glm = GLM("glm-4-plus", "test.jsonl")
    input_texts = ["Hello, how are you?", "What is the meaning of life?"]
    responses = glm.generate(input_texts)
    print(responses)
